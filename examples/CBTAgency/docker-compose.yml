version: "2.2"

services:
  controller:
    image: xinhai_backend
    volumes:
      - ../../backend:/usr/share/xinhai/backend
      - ./configs:/usr/share/xinhai/configs
    ports:
      - 8080:8080
      - ${CONTROLLER_PORT}:${CONTROLLER_PORT}
    environment:
      PYTHONPATH: /usr/share/xinhai/backend/src
    command: python -m xinhai.controller --host ${CONTROLLER_HOST} --port ${CONTROLLER_PORT}
    healthcheck:
      test:
        [
          "CMD", "curl", "--fail", "-s", "-X", "POST", "-H", "Content-Type: application/json", "-d", "{}",
          "http://localhost:${CONTROLLER_PORT}/api/list_models",
        ]
      interval: 10s
      timeout: 10s
      retries: 120
  frontend:
    image: xinhai_frontend
    volumes:
      - ../../frontend:/usr/share/xinhai/frontend
      - ./configs:/usr/share/xinhai/configs
    network_mode: service:controller
    working_dir: /usr/share/xinhai/frontend
    command: npm run serve
  storage:
    image: xinhai_backend
    depends_on:
      controller:
        condition: service_healthy
    volumes:
      - ../../backend:/usr/share/xinhai/backend
      - ./configs:/usr/share/xinhai/configs
      - pretrained_models:/usr/share/xinhai/pretrained_models
    environment:
      CONTROLLER_ADDRESS: http://controller:${CONTROLLER_PORT}
      MODEL_NAME: storage
      WORKER_ADDRESS: http://storage:${STORAGE_WORKER_PORT}
      WORKER_HOST: 0.0.0.0
      WORKER_PORT: ${STORAGE_WORKER_PORT}
      DB_PATH: /usr/share/xinhai/pretrained_models/${STORAGE_DB_PATH}
      EMBEDDING_MODEL_PATH: /usr/share/xinhai/pretrained_models/${STORAGE_EMBEDDING_MODEL_PATH}
      PYTHONPATH: /usr/share/xinhai/backend/src
    command: python -m xinhai.workers.storage
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [ '${LLM_DEVICE}' ]
              capabilities: [ gpu ]
    healthcheck:
      test: [
          "CMD", "curl", "--fail", "-s", "-X", "POST", "-H", "Content-Type: application/json", "-d", "{}",
          "http://localhost:${STORAGE_WORKER_PORT}/worker_get_status",
        ]
      interval: 10s
      timeout: 10s
      retries: 120
  llm:
    image: xinhai_backend
    depends_on:
      controller:
        condition: service_healthy
      storage:
        condition: service_healthy
    volumes:
      - ../../backend:/usr/share/xinhai/backend
      - ./configs:/usr/share/xinhai/configs
      - pretrained_models:/usr/share/xinhai/pretrained_models
    environment:
      CONTROLLER_ADDRESS: http://controller:${CONTROLLER_PORT}
      MODEL_NAME: ${LLM_MODEL_NAME}
      WORKER_ADDRESS: http://llm:${LLM_WORKER_PORT}
      WORKER_HOST: 0.0.0.0
      WORKER_PORT: ${LLM_WORKER_PORT}
      PYTHONPATH: /usr/share/xinhai/backend/src
    command: python -m xinhai.workers.llm --model_name_or_path /usr/share/xinhai/pretrained_models/${LLM_MODEL_PATH} --template ${LLM_MODEL_TEMPLATE} --infer_backend vllm --vllm_enforce_eager --infer_dtype float16
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [ '0' ]
              capabilities: [ gpu ]
    healthcheck:
      test: [
        "CMD", "curl", "--fail", "-s", "-X", "POST", "-H", "Content-Type: application/json", "-d", "{}",
        "http://localhost:${LLM_WORKER_PORT}/worker_get_status",
      ]
      interval: 10s
      timeout: 10s
      retries: 120
  #  llm: # Bridged LLM
  #    image: xinhai
  #    depends_on:
  #      - controller
  #      - storage
  #    volumes:
  #      - ./backend:/usr/share/xinhai/backend
  #      - ./configs:/usr/share/xinhai/configs
  #      - ./related_repos:/usr/share/xinhai/related_repos
  #    environment:
  #      PYTHONPATH: /usr/share/xinhai/related_repos/LLaMA-Factory/src
  #      CONTROLLER_ADDRESS: http://controller:${CONTROLLER_PORT}
  #      MODEL_NAME: Qwen/Qwen2-7B-Instruct
  #      WORKER_ADDRESS: http://bridge:${BRIDGE_WORKER_PORT}
  #      WORKER_HOST: 0.0.0.0
  #      WORKER_PORT: ${BRIDGE_WORKER_PORT}
  #      API_KEY: ${BRIDGE_API_KEY}
  #      API_BASE: ${BRIDGE_API_BASE}
  #    command: python -m xinhai.workers.bridge
  cbt_agency:
    image: xinhai_backend
    depends_on:
      controller:
        condition: service_healthy
      storage:
        condition: service_healthy
      llm:
        condition: service_healthy
    volumes:
      - ../../backend:/usr/share/xinhai/backend
      - ./configs:/usr/share/xinhai/configs
    environment:
      CONTROLLER_ADDRESS: http://controller:${CONTROLLER_PORT}
      MODEL_NAME: ${AGENCY_NAME}
      WORKER_ADDRESS: http://cbt_agency:${AGENCY_WORKER_PORT}
      WORKER_HOST: 0.0.0.0
      WORKER_PORT: ${AGENCY_WORKER_PORT}
      AGENCY_NAME: ${AGENCY_NAME}
      AGENCY_CONFIG_PATH: /usr/share/xinhai/configs/${AGENCY_NAME}.yaml
      PYTHONPATH: /usr/share/xinhai/backend/src
    command: python -m xinhai.workers.agency

volumes:
  backend:
    driver: local
  configs:
    driver: local
  pretrained_models:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: /data/pretrained_models
